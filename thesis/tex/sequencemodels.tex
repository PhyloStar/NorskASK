\chapter{Sequence models}

Next, we will implement models which take the ordering of tokens into
account. This includes \acp{CNN} and \acp{RNN}. In \ac{NLP}, \acp{CNN} can be
considered as a ``\ngram extractor'', which looks at a window of tokens.
However, unlike the count vector approach used above, the CNN extracts
``soft'' \ngrams, utilizing the ability of word embeddings to share
statistical strength between words, and outputting a real-valued number
instead of a boolean yes/no value or an integer count.

\acp{RNN} are theoretically able to detect long range patterns, which seems
like it would be useful for the tasks at hand. For instance, the ability to
connect different sections of a text and refer back to things that have been
mentioned before can be considered a part of language proficiency.


\section{Experimental setup}

Certain aspects of setup are shared between experiments using \ac{CNN} and
\ac{RNN} models.


\subsection{Input length}

The models in this chapter take as input a document of a predetermined
length. We therefore needed to set a fixed number of tokens such that shorter
documents were padded to this length, and longer documents truncated. In
order to decide this, we examined the distribution of document lengths in the
training set. All subsequent values are rounded to the nearest integer.
First, we found the 95th percentile of document lengths, which turned out to
be 701 tokens. Then, we computed the value $Q_2 + 1.5 \cdot (Q_3 - Q_1)$, or
the median plus 1.5 times the interquartile range, which gives 693 tokens.
Finally, we computed the mean value plus two standard deviations, giving 707
tokens. These values are all close to each other, and we decided to settle on
700 tokens since it is a round number close to all values we examined.

As a consequence of the unequal distributions of length between the two test
levels, the documents that are truncated are mainly from the AL test. This
could be problematic if the 


\subsection{Pre-trained embeddings}

We experimented with randomly initialized word embedding vectors trained from
scratch, as well as initializing the vectors with pre-trained embeddings.
We know that our corpus contains tokens with spelling mistakes, which are
likely to be absent in any pre-trained vector model. We therefore sought out
pre-trained models using the FastText algorithm, which lets us compute
vectors for words that do not have separate entries in the model, basing
the vector on the word's constituent character \ngrams.

In our case, we used a selection of models trained on a large Norwegian
corpus, the combination of Norsk aviskorpus (The Norwegian Newspaper Corpus)
and NoWaC (Norwegian Web As Corpus) \autocite{stadsnes2018}. These vector
models are very large, containing vectors for more than 2,500,000 words. The
models are stored in a repository that is available online and on the Abel
supercomputer cluster \autocite{murhaf2017repository}. We use three models
trained on this corpus using the fastText algorithm, differing only in the
dimension of embeddings. They were trained using skipgram and window size 5, 
and lemmatization has not been applied to the corpora.

Our training data only contains 20,766 unique tokens, which is less than 1\%
of the full vocabulary of the models. Loading the full pre-trained model
takes a long time and uses huge amounts of memory for word vectors we will
never use. For that reason, we created models of smaller size by loading the
full models once and iterating through all of the word forms in our corpus,
storing the resulting vectors in a new vector model containing only 20,766
words. In this way, we are also able to benefit from the FastText models'
ability to compute vectors for unknown words, since the \ngram algorithm is
being used when computing the reduced models. However, after the embedding
layers of our models have been initialized with vectors from the fastText
model, fine-tuning of vectors happens in the same way as if we had used
embeddings from any other model such as Word2vec, or even randomly
initialized embeddings, since the FastText network is not incorporated into
our models.

When using pre-trained embeddings, we ran some experiments keeping the
embeddings static, and in some other experiments we further fine-tuned the
embeddings using training signals from the task.


\subsection{Multi-channel input}

For some of our experiments we use both word tokens and their POS tags as
input at the same time. To do this we include two separate embedding matrices
in our network, one for words and one for POS tags. These embedding matrices
do not need to have the same dimensions, and because the number of
different POS tags is very low compared to the number of different words, we
use smaller vectors for POS tags.

To create the input to the core part of our network, we concatenate the word
and POS embeddings into a single vector. For instance, if we use word
embeddings of size 50 and POS embeddings of size 10, the next layer will
receive vectors of size 60.


\section{Convolutional neural networks}

We create a model with a convolutional architecture based on the model
described in \textcite{kim2014convolutional}. Documents are represented as
sequences of token IDs, and fed into an embedding lookup layer. A separate
token ID is used for padding if the document is shorter than 700 tokens.
Another unique token ID is used for unknown tokens, i.e. tokens that either
are not present in training data, or are not among the $n$ most frequent
tokens, if we select a frequency cutoff.

The central part of the architecture is a set of convolutional filter banks
that are applied to sequences of embeddings. We may use several different
window sizes for the filters. The default architecture from
\textcite{kim2014convolutional} uses 300 convolutional filters: 100 each of
window size 3, 4 and 5. After applying the convolutions, the output is max
pooled along the time axis. This selects the highest output each filter
computed across all windows in the document. In practice, three pooling
operations are included in the computational graph, one for each filter bank.
This is a technical consideration, necessary because of the different window
sizes. The pooled vectors for each of the filter banks are concatenated into
a single vector. This vector has as many elements as there are filters in all
the filter banks combined.

The vector post-pooling is a vector representation of the entire document,
and is fed to a final softmax layer to produce a classification output.
During training, we apply dropout to this final weight layer as a
regularization method.


\subsection{Results}

The \ac{CNN} classifier that used both tokens and POS tags as input performed
better than the one which only used tokens as input, as seen in table
\ref{tab:cnn-results}. However, on the collapsed label set, the model which
only used tokens had a higher accuracy. Filters of size 3, 4, and 5 were
used. The last dense layer uses dropout with $p=0.5$ and a constraint on
maximum $L_2$ norm of 3.

\begin{table}
  \centering
  \begin{tabular}{lrrrr}
    \toprule
            & \multicolumn{2}{c}{All labels}       & \multicolumn{2}{c}{Collapsed labels} \\
    \cmidrule(lr){2-3}
    \cmidrule(lr){4-5}
    Model     & Macro \FI        & Micro \FI        & Macro \FI        & Micro \FI \\
    \midrule
    % $BEGIN autotable cnn-results
    % $META models-per-row=2 columns-per-model=macrof1,microf1
    % $ROW Words:       cnn-26092298_01   cnn-26092298_02
    % $ROW Words+PTD:   cnn-26092298_03   cnn-26092298_04
    % $ROW Words+PTS:   cnn-26094553_05   cnn-26094553_06
    % $ROW +POS:        cnn-26092298_07   cnn-26092298_08
    % $ROW +POS+PTD:    cnn-26092298_09   cnn-26092298_10
    % $ROW +POS+PTS:    cnn-26094553_11   cnn-26094553_12
    % $ROW Mix:         cnn-26093545_13   cnn-26093545_14
    % $ROW Mix 234:     cnn-26093545_15   cnn-26093545_16
    % $END autotable
    Words & $0.226$ & $0.415$ & $0.397$ & $0.748$ \\
    Words+PTD & $0.238$ & $0.390$ & $0.375$ & $0.707$ \\
    Words+PTS & $0.235$ & $0.382$ & $\mathbf{0.404}$ & $0.699$ \\
    +POS & $0.228$ & $0.390$ & $0.402$ & $\mathbf{0.756}$ \\
    +POS+PTD & $0.229$ & $0.415$ & $0.388$ & $0.732$ \\
    +POS+PTS & $\mathbf{0.255}$ & $\mathbf{0.447}$ & $0.393$ & $0.740$ \\
    Mix & $0.244$ & $0.415$ & $0.358$ & $0.675$ \\
    Mix 234 & $0.239$ & $0.407$ & $0.401$ & $\mathbf{0.756}$ \\
    \bottomrule
  \end{tabular}
  \caption{\FI scores of CNN classifiers on AES. +POS: Multi-channel input with
           both words and POS tags. +PTD: Pre-trained word embeddings, further
           fine-tuning. +PTS: Pre-trained word embeddings kept static. 234:
           filter sizes 2, 3 and 4.}
  \label{tab:cnn-results}
\end{table}


\subsection{Training behaviour}

\begin{figure}
  % cnn-26094553_11
  \begin{subfigure}{\linewidth}
    \centering
    \includegraphics{cnn-training}
    \caption{Training and validation loss and accuracy over 50 epochs of training.}
  \end{subfigure}
  \begin{subfigure}{\linewidth}
    \centering
    \includegraphics{cnn-confusion}
    \caption{Confusion matrix on validation set, raw counts and normalized.}
  \end{subfigure}
  \caption{CNN with UPOS as side input, using pre-trained, static embedings.}
  \label{fig:cnn-training}
\end{figure}

Figure \ref{fig:cnn-training} shows that how the training and validation
metrics evolve in the course of training, as well as the confusion matrix of
the final model. The validation metrics plateau after approximately 20
epochs, after which any improvement is eclipsed by small, seemingly random
fluctuations. The training metrics keep improving, suggesting that the model
is memorizing the data rather than discovering general features. When
training stops after 50 epochs, the training accuracy is almost at 100\%.

The confusion matrix reveals that the final model has no predictions for the
classes `A2', `B2/C1' and `C1', the smallest classes in the validation set.


\section{Recurrent neural networks}

A \ac{LSTM} \ac{RNN} was implemented based on the architecture described in
\textcite{taghipour16}. Several changes to their architecture were made in
order to accommodate our data. For instance, \citeauthor{taghipour16} modelled
the task as a regression problem, their output layer consisting of a single
node with a value constrained to (0, 1) by the sigmoid function. This layer
was replaced with a softmax layer similar to the \acp{MLP} in section
\ref{subsec:mlp}.

We then chose a loss function to be compatible with a multi-class softmax
output. Like above, we used categorical cross-entropy (equation
\ref{eq:crossentropy}). A different evaluation metric was also needed because
we are treating the task as multi-class prediction. We are reporting macro
and micro \FI as before. We do not use the \ac{QWK} metric used by
\citeauthor{taghipour16}.

The formulation of the \ac{AES} task as a regression problem by
\citeauthor{taghipour16} was partly a constraint stemming from the Kaggle
competition that supplied the data and problem formulation, and partly
motivated by the nature of the data. The ASAP data that they use consists of
essays from eight different prompts, and the scoring methods differs across
prompts. Since the scores are numeric values over different ranges, modelling
the task as a regression problem made it sufficient to normalize the numeric
scores to a common interval before training.

\todo{remainder of section unstructured}
Generally for \ac{AES}, modelling the task as multi-class prediction is
common, and is used in \textcite{vajjala18universalCEFR}. In
\textcite{vajjala17}, two different datasets with different properties were
used, and the author utilized both multi-class prediction and regression at
different points.

The embedding layer in \textcite{taghipour16} was initialized with
pre-trained embeddings of size 50. We experiment with different sizes of
embeddings, as well as random vs. pre-trained initialization.

Unlike our corpus, ASK, the ASAP dataset used by \citeauthor{taghipour16}
contained essays that were not necessarily written in a second language. Our
data is not split into different parts based on the prompt. There are two
different test levels in ASK, but these are not distinguished in training.

Note that the activation function for the gates in gated RNNs in Keras by
default is the \emph{Hard sigmoid} activation function (ref. eq.
\ref{eq:hardsigmoid}), chosen because it is computationally more efficient
than the sigmoid function.

We also experimented with a different type of gated \ac{RNN} cell, the
\ac{GRU}. This variant of \ac{RNN} is slightly simpler than the \ac{LSTM},
featuring only two gates. However, it is still designed to be able to
propagate the error gradient over a larger number of timesteps, like
\ac{LSTM}. In this way the architecture is supposed to mitigate the problem
of vanishing or exploding gradients. The equations defining \ac{LSTM}
\ac{GRU} cells are listed in chapter \ref{ch:background}, equations
\ref{eq:lstm} and \ref{eq:gru}.


\subsection{Variants}

We attempt two different methods of combining the sequence of hidden states
from the \ac{RNN} into a feature vector. The simplest approach is \emph{mean
over time}, where we use the elementwise average of elements in hidden states
across the time dimension as our feature vector. The mean over time layer is
used in two of the experiments. Once with a \ac{RNN} processing the essay
from top to bottom, and once on top of a bidirectional RNN.

The bidirectional model (BiRNN) is constructed by running two \acp{RNN} over
the same input, but in the opposite order. The output from the BiRNN layer is
a sequence of vectors where, for each timestep $j$, the vector is the
concatenation of two vectors $[s_f;s_b]$ where $s_f$ at timestep $j$ is the
output from the forwards \ac{RNN} after processing the inputs $(x_1, x_2,
\ldots, x_j)$ and $s_b$ the output from the backwards \ac{RNN} after
processing the inputs $(x_m, x_{m-1}, \ldots, x_j)$, where $m$ is the total
number of timesteps. The BiRNN should therefore be able to extract context on
both sides of a input timestep.

\todo{Mean of the final state}

The other method is an attention layer, which differs from the mean over
time layer in that timesteps are weighted by an attention mechanism: a
single-layer neural network computes a value between -1 and 1 for each
timestep. These values are normalized by a softmax layer and then used to
compute the weighted average. Since each timestep contributes to the final
representation in differing amounts, the mechanism should be able in theory
to focus on crucial information by choosing weights such as to disregard
uninformative timesteps, improving performance. The attention mechanism is
trained along with the rest of the network.

All the models have a hidden state vector of size 300 in the \ac{RNN}. The
vocabulary was limited to the 4,000 most common words, the same vocabulary
size as is used in \textcite{taghipour16}. In our dataset, this is nearly
equivalent to using all tokens that occur at least 5 times: There are 4,031
tokens in our training set that occur at least 5 times.


\subsection{Results}

The RNN results are split into two tables. Table \ref{tab:lstm-results} contains
the results for models with \ac{LSTM} cells, while table \ref{tab:gru-results}
contains the results for models with \ac{GRU} cells.

\begin{table}
  \centering
  \begin{tabular}{lrrrr}
    \toprule
            & \multicolumn{2}{c}{All labels} & \multicolumn{2}{c}{Collapsed labels} \\
    \cmidrule(lr){2-3}
    \cmidrule(lr){4-5}
    Model     & Macro \FI      & Micro \FI      & Macro \FI      & Micro \FI \\
    \midrule
              \multicolumn{5}{c}{50 dimensional, randomly initialized embeddings} \\
    \midrule
    % $BEGIN autotable lstm-results
    % $META models-per-row=2 columns-per-model=macrof1,microf1
    % $ROW 1:           rnn-26285652_1       rnn-26285653_1
    % $ROW 2:           rnn-26285652_2       rnn-26285653_2
    % $ROW 3:           rnn-26285652_3       rnn-26285653_3
    % $ROW 4:           rnn-26285652_4       rnn-26285653_4
    % $ROW 5:           rnn-26285652_5       rnn-26285653_5
    % $ROW 6:           rnn-26285652_6       rnn-26285653_6
    % $ROW 7:           rnn-26285652_7       rnn-26285653_7
    % $ROW 8:           rnn-26285652_8       rnn-26285653_8
    % $ROW 9:           rnn-26285652_9       rnn-26285653_9
    % \midrule \multicolumn{5}{c}{Random init, BiRNN} \\ \midrule
    % $ROW 10:          rnn-26285652_10      rnn-26285653_10
    % $ROW 11:          rnn-26285652_11      rnn-26285653_11
    % $ROW 12:          rnn-26285652_12      rnn-26285653_12
    % $ROW 13:          rnn-26285652_13      rnn-26285653_13
    % $ROW 14:          rnn-26285652_14      rnn-26285653_14
    % $ROW 15:          rnn-26285652_15      rnn-26285653_15
    % $ROW 16:          rnn-26285652_16      rnn-26285653_16
    % $ROW 17:          rnn-26285652_17      rnn-26285653_17
    % $ROW 18:          rnn-26285652_18      rnn-26285653_18
    % \midrule \multicolumn{5}{c}{Pre-trained, unidirectional RNN} \\ \midrule
    % $ROW 19:          rnn-26285652_19      rnn-26285653_19
    % $ROW 20:          rnn-26285652_20      rnn-26285653_20
    % $ROW 21:          rnn-26285652_21      rnn-26285653_21
    % $ROW 22:          rnn-26285652_22      rnn-26285653_22
    % $ROW 23:          rnn-26285652_23      rnn-26285653_23
    % $ROW 24:          rnn-26285652_24      rnn-26285653_24
    % $ROW 25:          rnn-26285652_25      rnn-26285653_25
    % $ROW 26:          rnn-26285652_26      rnn-26285653_26
    % $ROW 27:          rnn-26285652_27      rnn-26285653_27
    % \midrule \multicolumn{5}{c}{Pre-trained, BiRNN} \\ \midrule
    % $ROW 28:          rnn-26285652_28      rnn-26285653_28
    % $ROW 29:          rnn-26285652_29      rnn-26285653_29
    % $ROW 30:          rnn-26285652_30      rnn-26285653_30
    % $ROW 31:          rnn-26285652_31      rnn-26285653_31
    % $ROW 32:          rnn-26285652_32      rnn-26285653_32
    % $ROW 33:          rnn-26285652_33      rnn-26285653_33
    % $ROW 34:          rnn-26285652_34      rnn-26285653_34
    % $ROW 35:          rnn-26285652_35      rnn-26285653_35
    % $ROW 36:          rnn-26285652_36      rnn-26285653_36
    % $END autotable
    \bottomrule
  \end{tabular}
  \caption{\FI scores of LSTM classifiers on AES}
  \label{tab:lstm-results}
\end{table}

\begin{table}
  \centering
  \begin{tabular}{lrrrr}
    \toprule
            & \multicolumn{2}{c}{All labels} & \multicolumn{2}{c}{Collapsed labels} \\
    \cmidrule(lr){2-3}
    \cmidrule(lr){4-5}
    Model     & Macro \FI      & Micro \FI      & Macro \FI      & Micro \FI \\
    \midrule
              \multicolumn{5}{c}{50 dimensional, randomly initialized embeddings} \\
    \midrule
    % $BEGIN autotable gru-results
    % $META models-per-row=2 columns-per-model=macrof1,microf1
    % $ROW 1:           rnn-26285656_1       rnn-26285657_1
    % $ROW 2:           rnn-26285656_2       rnn-26285657_2
    % $ROW 3:           rnn-26285656_3       rnn-26285657_3
    % $ROW 4:           rnn-26285656_4       rnn-26285657_4
    % $ROW 5:           rnn-26285656_5       rnn-26285657_5
    % $ROW 6:           rnn-26285656_6       rnn-26285657_6
    % $ROW 7:           rnn-26285656_7       rnn-26285657_7
    % $ROW 8:           rnn-26285656_8       rnn-26285657_8
    % $ROW 9:           rnn-26285656_9       rnn-26285657_9
    % \midrule \multicolumn{5}{c}{Random init, BiRNN} \\ \midrule
    % $ROW 10:          rnn-26285656_10      rnn-26285657_10
    % $ROW 11:          rnn-26285656_11      rnn-26285657_11
    % $ROW 12:          rnn-26285656_12      rnn-26285657_12
    % $ROW 13:          rnn-26285656_13      rnn-26285657_13
    % $ROW 14:          rnn-26285656_14      rnn-26285657_14
    % $ROW 15:          rnn-26285656_15      rnn-26285657_15
    % $ROW 16:          rnn-26285656_16      rnn-26285657_16
    % $ROW 17:          rnn-26285656_17      rnn-26285657_17
    % $ROW 18:          rnn-26285656_18      rnn-26285657_18
    % \midrule \multicolumn{5}{c}{Pre-trained, unidirectional RNN} \\ \midrule
    % $ROW 19:          rnn-26285656_19      rnn-26285657_19
    % $ROW 20:          rnn-26285656_20      rnn-26285657_20
    % $ROW 21:          rnn-26285656_21      rnn-26285657_21
    % $ROW 22:          rnn-26285656_22      rnn-26285657_22
    % $ROW 23:          rnn-26285656_23      rnn-26285657_23
    % $ROW 24:          rnn-26285656_24      rnn-26285657_24
    % $ROW 25:          rnn-26285656_25      rnn-26285657_25
    % $ROW 26:          rnn-26285656_26      rnn-26285657_26
    % $ROW 27:          rnn-26285656_27      rnn-26285657_27
    % \midrule \multicolumn{5}{c}{Pre-trained, BiRNN} \\ \midrule
    % $ROW 28:          rnn-26285656_28      rnn-26285657_28
    % $ROW 29:          rnn-26285656_29      rnn-26285657_29
    % $ROW 30:          rnn-26285656_30      rnn-26285657_30
    % $ROW 31:          rnn-26285656_31      rnn-26285657_31
    % $ROW 32:          rnn-26285656_32      rnn-26285657_32
    % $ROW 33:          rnn-26285656_33      rnn-26285657_33
    % $ROW 34:          rnn-26285656_34      rnn-26285657_34
    % $ROW 35:          rnn-26285656_35      rnn-26285657_35
    % $ROW 36:          rnn-26285656_36      rnn-26285657_36
    % $END autotable
    \bottomrule
  \end{tabular}
  \caption{\FI scores of GRU classifiers on AES}
  \label{tab:gru-results}
\end{table}

We find that the best performing models all use the attention mechanism.
Keep in mind that the stopping criterion is the macro \FI score, and thus
the micro \FI score is to be considered a side effect of the macro \FI.


\subsection{Training behaviour}

\begin{figure}
  % rnn-25858209
  \begin{subfigure}{\linewidth}
    \centering
    \includegraphics{rnn-training}
    \caption{Training and validation loss and accuracy over 50 epochs of training.}
  \end{subfigure}
  \begin{subfigure}{\linewidth}
    \centering
    \includegraphics{rnn-confusion}
    \caption{Confusion matrix on validation set, raw counts and normalized.}
  \end{subfigure}
  \caption{RNN with GRU cells, attention mechanism, and pre-trained embeddings
           of dimension 50, fine-tuned.}
  \label{fig:rnn-training}
\end{figure}

We see the training and classification of the best RNN model in figure
\ref{fig:rnn-training}, namely the GRU cell with pre-trained, fine-tuned
embeddings and an attention mechanism. It is plain to see that the model
overfits while the validation performance fluctuates. The validation metrics
seem to fluctuate more than they did with the CNN model in fig
\ref{fig:cnn-training}. However, the training accuracy seems to increase
slower, and does not quite reach 100\% in the course of 50 epochs.


\section{Native language identification}

Unlike the proficiency labels in ASK, \ac{L1}s are rather evenly distributed.

We train the same models to classify the documents by native language. The
performance of a \ac{CNN} model improved drastically when including \ac{POS}
tags as input, as evident in table \ref{tab:cnn-nli-results}.

A RNN was able to outperform the CNN only slightly, and here it was not the
attention model that was best, but a bidirectional LSTM.

\begin{table}
  \centering
  \begin{tabular}{lrr}
    \toprule
    Model     & Macro \FI      & Micro \FI \\
    \midrule
    Tokens    &         $0.367$  &         $0.366$  \\ % cnn-nli-2019-02-05_12-54-51
    +POS      & $\mathbf{0.467}$ & $\mathbf{0.463}$ \\ % cnn-nli-2019-01-30_15-32-28
    Mixed POS &         $0.336$  &         $0.333$  \\ % cnn-nli-02-11_16-15-54
    \bottomrule
  \end{tabular}
  \caption{\FI scores of CNN classifiers on NLI}
  \label{tab:cnn-nli-results}
\end{table}

\begin{table}
  \centering
  \begin{tabular}{lrr}
    \toprule
    Model     & Macro \FI      & Micro \FI \\
    \midrule
    Mean/Time &         $0.379$  &         $0.390$  \\ % rnn_nli-25740792
    BiLSTM    & $\mathbf{0.468}$ & $\mathbf{0.480}$ \\ % rnn_nli-25740786
    Attention &         $0.423$  &         $0.407$  \\ % rnn_nli-25740789
    \bottomrule
  \end{tabular}
  \caption{\FI scores of RNN classifiers on NLI}
  \label{tab:rnn-nli-results}
\end{table}

The attention model allows us to visualize the weights the network gives to
each token in a document. In figures
\ref{fig:h0186-nli-attention}--\ref{fig:s0621-nli-attention} we see up to 300
tokens of texts from four different documents in the dev set for which the L1
was correctly predicted by an attention model. Red tokens indicate timesteps
that were given higher weight by the attention model, and blue tokens ones
that were given low weights. Out of vocabulary tokens are replaced by the
special token ``UNK''. The attention values do not seem easily interpretable,
as there are lexical and grammatical errors both in the red and the blue
sections.

Looking for cues as to what features are picked up by the model. \todo{expand analysis}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{h0186-nli-attention}
  \caption{Attention values of NLI classifier on excerpt from ASK text h0186.
           L1 is Russian, CEFR score B2}
  \label{fig:h0186-nli-attention}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{h0189-nli-attention}
  \caption{Attention values of NLI classifier on excerpt from ASK text h0189.
           L1 is English, CEFR score C1}
  \label{fig:h0189-nli-attention}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{s0180-nli-attention}
  \caption{Attention values of NLI classifier on excerpt from ASK text s0180.
           L1 is Vietnamese, CEFR score A2/B1}
  \label{fig:s0180-nli-attention}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{s0621-nli-attention}
  \caption{Attention values of NLI classifier on full ASK text s0621.
           L1 is Somali, CEFR score A2/B1}
  \label{fig:s0621-nli-attention}
\end{figure}
\todo{Visualize UPOS tags}


\section{Visualization of latent space}

We can use dimensionality reduction methods such as $t$-SNE in order to see
if the intermediate representation of documents prior to the final
classification layer positions documents that should be similar close to each
other.

\begin{figure}
  \centering
  % \includegraphics[width=\textwidth]{cefr-t-sne}
  \caption{$t$-SNE plot of the vector representations of documents}
  \label{fig:cefr-t-sne}
\end{figure}
