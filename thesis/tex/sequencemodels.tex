\chapter{Further experiments}

\section{Sequence based models}

Next, we will implement models which take the ordering of tokens into account.
This includes \acp{CNN} and \acp{RNN}. 

\subsection{LSTM}

A \ac{LSTM} \ac{RNN} was implemented based on the architecture described
by \textcite{taghipour16}.

Differences between the task set ups include the following:

\begin{enumerate}
    \item \citeauthor{taghipour16} modelled the task as a regression problem,
        and their output layer consisted of a single node with a value constrained
        to (0, 1) by the sigmoid function. This layer was replaced with a softmax
        layer similar to the \acp{MLP} in section \ref{subsec:mlp}.
    \item The loss function needed to be compatible with a multi-class softmax
        output. We used categorical cross-entropy.
    \item Unlike our corpus, ASK, \citeauthor{taghipour16} used essays that were
        not necessarily written in a second language.
    \item Our data is not split into different parts based on the prompt. There
        are two different test levels in ASK, though.
    \item A different evaluation metric was needed because we are treating the task
        as multi-class evaluation. The \ac{QWK} metric is not applicable to
        the predictions of our classifier.
\end{enumerate}

\subsection{Variants}

We attempt two different methods of combining the sequence of hidden states
from the \ac{LSTM} into a feature vector. The simplest approach is \emph{mean
over time}, where we use the elementwise average of elements in hidden states
as our feature vector.

The attention model uses an attention layer, which instead computes a
weighted sum of the states. In order to find the weight to apply to each
state, a single-layer neural network computes a value between -1 and 1 for
each timestep. These values are normalized by a softmax layer and then used
to compute the weighted average. The neural network is trained along with the
rest of the network.

\subsection{Results}

Results for four different models are shown in table \ref{lstm-results}. All
but \emph{Modified} are using the same hyperparameters as
\citeauthor{taghipour16}. The \emph{Attention} model does not use masking,
while \emph{Mean/Time} and \emph{BiLSTM} do.

\begin{table}
  \centering
  \begin{tabular}{|l|rr|rr|}
    \toprule
            & \multicolumn{2}{c|}{All labels} & \multicolumn{2}{c|}{Collapsed labels} \\
    Model     & Macro F1        & Micro F1        & Macro F1        & Micro F1        \\
    \midrule
    Modified  &         26.7\%  & \textbf{46.3\%} &         49.6\%  &         72.4\%  \\
    Mean/Time &         28.0\%  &         38.2\%  &         46.1\%  &         69.9\%  \\
    BiLSTM    &         28.6\%  &         39.8\%  &         48.5\%  &         68.3\%  \\
    Attention & \textbf{29.2\%} &         40.7\%  & \textbf{54.7\%} & \textbf{75.6\%} \\
    \bottomrule
  \end{tabular}
  \caption{F1 scores of LSTM classifiers}
  \label{lstm-results}
\end{table}

The attention model performs best. While the modified model has the highest
micro F1, the stopping criterion is based on the Macro F1, and the micro
F1 scores are not necessarily comparable.
