\chapter{Sequence models}

Next, we will implement models which take the ordering of tokens into account.
This includes \acp{CNN} and \acp{RNN}. 


\section{Convolutional neural networks}

We create a model with convolutional architecture based on the model
described in \textcite{zhang2017sensitivity}. The basis of this architecture is
a set of filter banks with varying window size. After applying the convolutions,
the output is pooled along the time axis, leaving a vector with as many elements
as there are filters in the filter banks. The vectors for each of the filter
banks are concatenated into a single vector.


\subsection{Results}

\begin{table}
  \centering
  \begin{tabular}{|l|rr|rr|}
    \toprule
            & \multicolumn{2}{c|}{All labels} & \multicolumn{2}{c|}{Collapsed labels} \\
    Model     & Macro F1        & Micro F1        & Macro F1        & Micro F1        \\
    \midrule
    Tokens    &         0000\%  &         0000\%  &         0000\%  &         0000\%  \\
    +POS      & \textbf{28.3\%} & \textbf{43.1\%} &         0000\%  &         0000\%  \\
    \bottomrule
  \end{tabular}
  \caption{F1 scores of CNN classifiers on AES}
  \label{cnn-results}
\end{table}

\begin{table}
  \centering
  \begin{tabular}{|l|rr|}
    \toprule
    Model     & Macro F1        & Micro F1        \\
    \midrule
    Tokens    &         0000\%  &         0000\%  \\
    +POS      & \textbf{46.7\%} & \textbf{46.3\%} \\
    \bottomrule
  \end{tabular}
  \caption{F1 scores of CNN classifiers on NLI}
  \label{cnn-nli-results}
\end{table}


\section{Recurrent neural networks}

A \ac{LSTM} \ac{RNN} was implemented based on the architecture described by
\textcite{taghipour16}. Several changes to their architecture were made in
order to accomodate our data. For instance, \citeauthor{taghipour16} modelled
the task as a regression problem, their output layer consisting of a single
node with a value constrained to (0, 1) by the sigmoid function. This layer
was replaced with a softmax layer similar to the \acp{MLP} in section
\ref{subsec:mlp}. The loss function needed to be compatible with a
multi-class softmax output. We used categorical cross-entropy. A different
evaluation metric was also needed because we are treating the task as
multi-class prediction. We are reporting macro and micro F1 as before. The
\ac{QWK} metric used by \citeauthor{taghipour16} is not applicable to the
predictions of our classifier.

The embedding layer in \textcite{taghipour16} was initialized with
pre-trained embeddings of size 50. Our embedding layer was initialized to
random vectors and trained as part of the network.

Unlike our corpus, ASK, the dataset used by \citeauthor{taghipour16}
contained essays that were not necessarily written in a second language. Our
data is not split into different parts based on the prompt. There are two
different test levels in ASK, but these are not distinguished in training.

\begin{equation}
  \begin{aligned}
  s_j = R_{LSTM}(s_{j-1}, x_j) &= [c_j;h_j] \\
                           c_j &= f \odot c_{j-1} + i \odot z \\
                           h_j &= o \odot \tanh(c_j) \\
                             i &= \sigma(x_j W^{xi} + b_i + h_{j-1} W^{hi}) \\
                             f &= \sigma(x_j W^{xf} + b_f + h_{j-1} W^{hf}) \\
                             o &= \sigma(x_j W^{xo} + b_o + h_{j-1} W^{ho}) \\
                             z &= \tanh(x_j W^{xz} + b_z + h_{j-1} W^{hz}) \\
                             \\[1\jot]
                 O_{LSTM}(s_j) &= h_j
  \end{aligned}
\end{equation}
\todo{Should the equations be in Background chapter?}
\todo{Should vectors/matrices use bold font?}

where $\sigma$ by default in Keras is the \emph{Hard sigmoid} activation
function, chosen because it is computationally more efficient than the proper
sigmoid function:

\begin{equation}
  \sigma(x) = \begin{cases}
    0                 & : x < -2.5 \\
    0.2 \cdot x + 0.5 & : -2.5 \leq x \leq 2.5 \\
    1                 & : x > 2.5
  \end{cases}
\end{equation}

We also experimented with a different type of \ac{RNN} cell, the \ac{GRU}. This
variant of \ac{CNN} is a little simpler than the \ac{LSTM}, featuring only two
gates. However, it is still designed to be able to propagate the error gradient
over a larger number of timesteps and is supposed to solve the same problems
as \ac{LSTM}. The equations defining a \ac{GRU} layer are defined below:

\begin{equation}
  \begin{aligned}
  s_j = R_{GRU}(s_{j-1}, x_j) &= (1 - z) \odot s_{j-1} + z \odot \tilde{s_{j}} \\
                            r &= \sigma(x_j W^{xi} + b_i + h_{j-1} W^{hi}) \\
                            z &= \sigma(x_j W^{xf} + b_f + h_{j-1} W^{hf}) \\
                 \tilde{s{j}} &= \tanh(x_j W^{xs} + b_z + (r \odot s{j-1}) W^{sg}) \\
  \\[1\jot]
                 O_{GRU}(s_j) &= s_j
  \end{aligned}
\end{equation}


\subsection{Variants}

We attempt two different methods of combining the sequence of hidden states
from the \ac{LSTM} into a feature vector. The simplest approach is \emph{mean
over time}, where we use the elementwise average of elements in hidden states
across the time dimension as our feature vector. The mean over time layer is
used in two of the experiments. Once with a \ac{LSTM} processing the essay
from top to bottom, and once on top of a bidirectional LSTM (BiLSTM). In the
BiLSTM, the output from the \ac{RNN} on each timestep is the concatenation of
the states of two \acp{LSTM}, one processing the document from the top and
the other one from the bottom up.

\todo{A paragraph on BiLSTM}

The attention model uses an attention layer, which instead computes a
weighted sum of the states, and therefore should be able in theory to
disregard uninformative timesteps and improve performance. In order to find
the weight to apply to each state, a single-layer neural network computes a
value between -1 and 1 for each timestep. These values are normalized by a
softmax layer and then used to compute the weighted average. The attention
mechanism is trained along with the rest of the network. The attention model
uses a unidirectional \ac{LSTM}.

The modified model differs in which parts of the network uses dropout, as
well as in certain hyperparameters. All the models use word embedding vectors
of size 50 and a hidden state vector of size 300 in the \ac{LSTM}. The
vocabulary was limited to the 4,000 most common words.

\subsection{Results}

Results for four different models are shown in table \ref{lstm-results}. All
but \emph{Modified} are using the same hyperparameters as
\citeauthor{taghipour16}. The \emph{Attention} model does not use masking,
while \emph{Mean/Time} and \emph{BiLSTM} do.

\begin{table}
  \centering
  \begin{tabular}{|l|rr|rr|}
    \toprule
            & \multicolumn{2}{c|}{All labels} & \multicolumn{2}{c|}{Collapsed labels} \\
    Model     & Macro F1        & Micro F1        & Macro F1        & Micro F1        \\
    \midrule
    Modified  &         26.7\%  & \textbf{46.3\%} &         49.6\%  &         72.4\%  \\
    Mean/Time &         28.0\%  &         38.2\%  &         46.1\%  &         69.9\%  \\
    BiLSTM    &         28.6\%  &         39.8\%  &         48.5\%  &         68.3\%  \\
    Attention &         29.2\%  &         40.7\%  &         54.7\%  &         75.6\%  \\
    PreTrain  & \textbf{30.3\%} &         42.3\%  & \textbf{55.8\%} & \textbf{80.5\%} \\
    \bottomrule
  \end{tabular}
  \caption{F1 scores of LSTM classifiers}
  \label{lstm-results}
\end{table}

The attention model performs best. While the modified model has the highest
micro F1, the stopping criterion is based on the macro F1, and the micro
F1 scores are not necessarily comparable\textemdash the ranking of models might be
different if the stopping criterion was mased on micro F1 instead of macro.

\todo{Analysis of attention model}

\section{Native language identification}

We train the same models to classify the documents by native language.
