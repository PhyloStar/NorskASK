\chapter{Further experiments}

\section{Sequence based models}

Next, we will implement models which take the ordering of tokens into account.
This includes \acp{CNN} and \acp{RNN}. 

\subsection{LSTM}

A \ac{LSTM} \ac{RNN} was implemented based on the architecture described
by \textcite{taghipour16}.

Differences between the task set ups include the following:

\begin{enumerate}
    \item \citeauthor{taghipour16} modelled the task as a regression problem,
        and their output layer consisted of a single node with a value constrained
        to (0, 1) by the sigmoid function. This layer was replaced with a softmax
        layer similar to the \acp{MLP} in section \ref{subsec:mlp}.
    \item The loss function needed to be compatible with a multi-class softmax
        output. We used categorical cross-entropy.
    \item Unlike our corpus, ASK, \citeauthor{taghipour16} used essays that were
        not necessarily written in a second language.
    \item Our data is not split into different parts based on the prompt. There
        are two different test levels in ASK, though.
    \item A different evaluation metric was needed because we are treating the task
        as multi-class evaluation. The \ac{QWK} metric is not applicable to
        the predictions of our classifier.
\end{enumerate}

Results using the same hyperparameters as \citeauthor{taghipour16} are shown
in table \ref{lstm-results}.

\begin{table}
  \centering
  \begin{tabular}{|l|rr|rr|}
    \toprule
             & \multicolumn{2}{c|}{All labels} & \multicolumn{2}{c|}{Collapsed labels} \\
    Model      & Macro F1        & Micro F1        & Macro F1        & Micro F1        \\
    \midrule
    Simple TM  &         26.7\%  & \textbf{46.3\%} & \textbf{49.6\%} & \textbf{72.4\%} \\
    Hyper TM   & \textbf{28.0\%} &         38.2\%  &         46.1\%  &         69.9\%  \\
    \bottomrule
  \end{tabular}
  \caption{F1 scores of LSTM classifiers}
  \label{lstm-results}
\end{table}
