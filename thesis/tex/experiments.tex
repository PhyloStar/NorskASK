
\section{Preprocessing}

The data files in the ASK corpus are in XML format, and contain information
about tags, mistakes and corrections, paragraphs, sentences and more. These
files were transformed into other formats during the process. First, they
were converted to plain text files stripped of all tags or correction labels,
with one sentence per line consisting of space-separated tokens, and an empty
line separating paragraphs.

These raw text files were then sent through the UDPipe pipeline for tagging
and dependency parsing. The output from UDPipe is in the CoNLL file format
with a single token per line. UDPipe tags the documents using the UD tagset,
while the original tags in the XML documents are from the Oslo-Bergen
tagger's own tagset.

\section{Metrics}

Because of the unbalanced nature of the classes in the dataset, special
consideration has to be made as to the metrics of evaluation. Two metrics are
reported for all experiments: The macro average F1 and the micro average F1.
The latter is equivalent to the accuracy (ratio of correctly predicted
samples), while the former gives all the classes equal weight. If a model
correctly classifies most samples in the classes with most samples, but gives
poor predictions for smaller classes, then the macro F1 score will be
considerably lower than the micro F1 score.

The metrics are reported for two different modes of training and evaluation.
The first is training and evaluating using the full set of classes, and the
second is to train and evaluate on the collapsed classes.

A third option, namely to train on the full set of classes and reduce the
predictions to the collapsed set of classes, was also attempted, based on the
assumption that the more fine grained labels in the full set of classes can
provide useful training signals even though we evaluate on a smaller set.
However, in practice the best performance on the collapsed tag set was
empirically observed from the models that were also trained on the collapsed
tags.

\section{Baseline}

Two different sets of classes are used in the experiments. The original seven
CEFR labels, and a collapsed set where the intermediate classes such as
``A2/B1'' are rounded up to the nearest canonical class. This results in only
four different labels.

The majority class in the training set is B1. A majority classifier gets an
accuracy of 18.7\% on the test set using non-collapsed labels. With the
collapsed labels, the majority class is still B1, and the accuracy on the
test set is 34.1\%.

Moving to a simple bag-of-words model, a logistic regression classifier
achieves an accuracy of 28.5\% on the dev set without collapsed labels and
58.5\% with collapsed labels. There are approximately 18,300 different word
forms in the training set.

Several neural network models were attempted as well, with input either being
word counts, character \ngrams, or part of speech \ngrams. For the \ngram
features, \textit{n}s in the interval [2, 4] were used, and only the 10,000
most common features were kept.

All results are seen in table \ref{baseline-accuracies}.

\begin{table}
  \centering
  \begin{tabular}{|l|rr|rr|}
    \toprule
      & \multicolumn{2}{|l|}{All labels} & \multicolumn{2}{|c|}{Collapsed labels} \\
    Model      & Macro F1 & Micro F1 & Macro F1 & Micro F1 \\
    \midrule
    Majority   &    4.0\% &   16.3\% &   12.7\% &   34.1\% \\
    LogReg BOW &   17.9\% &   28.5\% &   31.9\% &   58.5\% \\
    MLP BOW    &   22.1\% &   35.0\% &   41.0\% &   64.2\% \\
    MLP Char   &   20.3\% &   36.6\% &   36.8\% &   70.7\% \\
    MLP POS    &   24.5\% &   39.0\% &   42.8\% &   65.9\% \\
    \bottomrule
  \end{tabular}
  \caption{Accuracy of different classifiers}
  \label{baseline-accuracies}
\end{table}

The part of speech \ngrams performed best overall, having the highest F1 score
for the full label set, both for macro and micro average. On the collapsed set
of labels, the part of speech \ngrams had the highest macro F1 score, but were
beat in micro F1 by the character \ngrams.

A convolutional neural network based on the architecture in
\textcite{zhang2017sensitivity} achieved an accuracy of 42.3\% using
sequences of POS tags as input to an initial embedding layer.
