
\section{Preprocessing}

The data files in the ASK corpus are in XML format, and contain information
about tags, mistakes and corrections, paragraphs, sentences and more. These
files were transformed into other formats during the process. First, they
were converted to plain text files stripped of all tags or correction labels,
with one sentence per line consisting of space-separated tokens, and an empty
line separating paragraphs.

These raw text files were then sent through the UDPipe pipeline for tagging
and dependency parsing. The output from UDPipe is in the CoNLL file format
with a single token per line. UDPipe tags the documents using the UD tagset,
while the original tags in the XML documents are from the Oslo-Bergen
tagger's own tagset.

\section{Metrics}

Because of the unbalanced nature of the classes in the dataset, special
consideration has to be made as to the metrics of evaluation. Along with
accuracy, three different F1 metrics are reported: The macro average, micro
average and weighted average.

These are reported for three different modes of training and evaluation. The
first is training and evaluating using the full set of classes, the second to
train on the full set of classes and reduce the predictions to the collapsed
set of classes, and the third is to train and evaluate on the collapsed
classes.

The second mode is based on the assumption that the more fine grained labels
in the full set of classes can provide useful training signals even though we
evaluate on a smaller set.

\section{Baseline}

Two different sets of classes are used in the experiments. The original seven
CEFR labels, and a collapsed set where the intermediate classes such as
"A2/B1" are rounded up to the nearest canonical class. This results in only
four different labels.

The majority class in the training set is B1. A majority classifier gets an
accuracy of 18.7\% on the test set using non-collapsed labels. With the
collapsed labels, the majority class is still B1, and the accuracy on the
test set is 34.1\%.

Moving to a simple bag-of-words model, a logistic regression classifier
achieves an accuracy of 28.5\% on the dev set without collapsed labels and
58.5\% with collapsed labels. There are approximately 18,300 different word
forms in the training set.

Several neural network models were attempted as well, with input either
being word counts, character \ngrams, or part of speech \ngrams.  For
the \ngram features, \textit{n}s in the interval [2, 4] were used, and
only the 10,000 most common features were kept.

All results are seen in table \ref{baseline-accuracies}.

\begin{table}
  \centering
  \begin{tabular}{lrr}
    \toprule
    Model            & All labels & Collapsed labels \\
    \midrule
    Majority         &     18.7\% &           34.1\% \\
    LogReg BOW       &     28.5\% &           58.5\% \\
    LogReg TL+Length &     43.9\% &           ------ \\
    MLP BOW          &     31.7\% &           63.4\% \\
    MLP Char         &     44.7\% &           68.3\% \\
    MLP POS          &     34.1\% &           65.0\% \\
    \bottomrule
  \end{tabular}
  \caption{Accuracy of different classifiers}
  \label{baseline-accuracies}
\end{table}

A logistic regression classifier was able to predict the CEFR score with
43.9\% accuracy using only two features: The length of the document, in
number of tokens, and the test level (IL test or AL test).

A convolutional neural network based on the architecture in
\textcite{zhang2017sensitivity} achieved an accuracy of 42.3\% using
sequences of POS tags as input to an initial embedding layer.
