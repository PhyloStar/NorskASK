
\section{Preprocessing}

The data files in the ASK corpus are in XML format, and contain information
about tags, mistakes and corrections, paragraphs, sentences and more. These
files were transformed into several other formats during the process. First,
they were converted to plain text files, stripped of all tags or correction
labels. The text files have one sentence per line, consisting of
space-separated tokens, and an empty line separating paragraphs.

These raw text files were then sent through the UDPipe pipeline
\autocite{udpipe:2017} for tagging and dependency parsing. The UDPipe project
maintains a REST API containing a selection of pretrained models. All
documents were transformed by the REST API using what was at the time of
writing the newest Norwegian bokmål (nb) model, available, namely
\texttt{norwegian-bokmaal-ud-2.3-181115}.

The pipeline accepts raw text files as input, where each sentence is put on a
separate line. The output from UDPipe is in the CoNLL file format, with a
single token per line. UDPipe tags the documents using the UD tagset, while
the original tags in the XML documents are from the Oslo-Bergen tagger's own
tagset.

\section{Metrics}

Because of the unbalanced nature of the classes in the dataset, special
consideration has to be made as to the metrics of evaluation. Two metrics are
reported for all experiments: The macro average F1 and the micro average F1.
The latter is equivalent to the accuracy (ratio of correctly predicted
samples), while the former gives all the classes equal weight. If a model
correctly classifies most samples in the classes with most samples, but gives
poor predictions for smaller classes, then the macro F1 score for the model
will be considerably lower than the micro F1 score.

The metrics are reported for two different modes: The first utilizing the
full set of classes, and the second training and evaluating on the collapsed
classes.

A third option, namely to train on the full set of classes and reduce the
predictions to the collapsed set of classes, was also attempted, based on the
assumption that the more fine grained labels in the full set of classes can
provide useful supervision signals even though we evaluate on a smaller set.
However, in practice the best perforers on the collapsed labels was
empirically observed to be the models that were also trained on the collapsed
tags.

\section{Model descriptions}

Initial experiments were run using logistic regression, which is a linear
classification method, and \acp{MLP}, which are simple neural networks.

\subsection{Logistic regression}

The logistic regression model was implemented using the Python library
Scikit-Learn \autocite{scikit-learn}. The model was instantiated with the
`lgbfs' solver and with multinomial as the multiclass method, and otherwise
used the default values for parameters.

This model used word count vectors as features. All tokens in the training set
were included.

\subsection{Multi-layer perceptron}
\label{subsec:mlp}

The MLP models were implementing in Keras \autocite{keras} and run on the
TensorFlow backend \autocite{tensorflow}. The models all have an input layer
with 10,000 dimensions, then two fully connected layers of size 256. The
fully connected layers use \ac{ReLU} activation and dropout regularization
with a dropout rate of 50\%. At the end, there is an output layer with
softmax activation and 7 or 4 dimensions, depending on whether we run with
collapsed labels or not.

The model was trained using the Adam optimization algorithm to minimize the
categorical cross-entropy loss. The learning rate was set to $2\cdot
10^{-4}$.

The neural network models were used with three different input features. For
word count vectors, the input is identical as for the logistic regression,
except that the number of features is limited to the 10,000 most common. 
The model was only used with \ac{POS} and character \ngrams. Here, all 
\textit{n}s in the interval [2, 4] were extracted, and only the 10,000 most
common of them kept.

\section{Results}

Two different sets of classes are used in the experiments. The original seven
CEFR labels, and a collapsed set where the intermediate classes, such as
``A2/B1'', are rounded up to the nearest canonical class, i.e. the CEFR label
right of the slash. This results in only four different labels: ``A2'',
``B1'', ``B2'' and ``C1''.

As a dumb baseline, we consider the majority class classifier. The majority
class in the training set is ``B1'', whether we consider the full class set
or the collapsed set. A majority classifier gets an accuracy of 18.7\% on the
test set using non-collapsed labels. With the collapsed labels, the accuracy
on the test set is 34.1\%. The macro F1 scores are much lower, as the
majority class classifier predicts no samples for any other classes.

Moving to a linear model, a logistic regression classifier using only
bag-of-word features achieves an accuracy of 28.5\% on the dev set without
collapsed labels and 58.5\% with collapsed labels. There are approximately
18,300 different word forms in the training set, and therefore the same
number of features in the bag-of-word model.

All results are seen in table \ref{baseline-accuracies}.

\begin{table}
  \centering
  \begin{tabular}{|l|rr|rr|}
    \toprule
             & \multicolumn{2}{c|}{All labels} & \multicolumn{2}{c|}{Collapsed labels} \\
    Model      & Macro F1        & Micro F1        & Macro F1        & Micro F1       \\
    \midrule
    Majority   &          4.0\%  &         16.3\%  &         12.7\%  &         34.1\% \\
    LogReg BOW &         17.9\%  &         28.5\%  &         31.9\%  &         58.5\% \\
    MLP BOW    &         22.1\%  &         35.0\%  &         41.0\%  &         64.2\% \\
    MLP Char   &         20.3\%  &         36.6\%  &         36.8\%  & \textbf{70.7\%} \\
    MLP POS    & \textbf{25.4\%} & \textbf{39.0\%} & \textbf{42.5\%} &         69.1\% \\
    \bottomrule
  \end{tabular}
  \caption{F1 scores of different classifiers}
  \label{baseline-accuracies}
\end{table}

The part of speech \ngrams performed best overall, having the highest F1 score
for the full label set, both for macro and micro average. On the collapsed set
of labels, the part of speech \ngrams had the highest macro F1 score, but were
beat in micro F1 by the character \ngrams.

Confusion matrices from MLP POS:

\begin{table}
  \centering
  \begin{tabular}{|l|rrrrrrr|}
    \toprule
          & A2 & A2/B1 & B1 & B1/B2 & B2 & B2/C1 & C1 \\
    \midrule
    A2    &  0 &     0 &  1 &     0 &  0 &     0 &  0 \\
    A2/B1 &  0 &     6 & 12 &     2 &  2 &     0 &  0 \\
    B1    &  0 &     2 & 13 &     3 &  2 &     0 &  0 \\
    B1/B2 &  0 &     1 & 10 &    11 &  8 &     0 &  0 \\
    B2    &  0 &     0 & 10 &     9 & 17 &     1 &  0 \\
    B2/C1 &  0 &     0 &  2 &     2 &  5 &     1 &  0 \\
    C1    &  0 &     0 &  0 &     1 &  1 &     1 &  0 \\
    \bottomrule
  \end{tabular}
  \caption{Confusion matrix of POS MLP on all labels}
  \label{confusion-full}
\end{table}

\begin{table}
  \centering
  \begin{tabular}{|l|rrrr|}
    \toprule
       & A2 & B1 & B2 & C1 \\
    \midrule
    A2 &  0 &  1 &  0 &  0 \\
    B1 &  0 & 36 &  6 &  0 \\
    B2 &  0 & 19 & 47 &  1 \\
    C1 &  0 &  1 & 10 &  2 \\
    \bottomrule
  \end{tabular}
  \caption{Confusion matrix of POS MLP on collapsed labels}
  \label{confusion-collapsed}
\end{table}

begrepet AES: L1 vs L2

Taghipour and Ng:
brukte de CNN foran LSTM eller ikke?
ulike numeriske karakterskalaer
evalueringer separert basert på prompt
regresjon
