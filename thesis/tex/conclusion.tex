\chapter{Conclusion} \label{ch:conclusion}

We have presented the first results for the \ac{AES} task on the ASK corpus
of Norwegian learner language. We have explored a wide variety of classifier
architectures, including linear models, simple neural networks (\acp{MLP})
and more specialized neural architectures: \acp{CNN} and \acp{RNN}. For this
dataset, we found that regression gave better results than nominal
classification. We further showed that different evaluation metrics commonly
used have a complex relationship with each other, and that the most suitable
metric to use may depend on the setting.

In chapter \ref{ch:background} we gave a brief introduction to machine
learning in general, and neural architectures in particular. We discussed
unique properties of learner language data, and introduced \ac{AES} and
\ac{NLI} as specific \ac{NLP} tasks relating to this kind of data. We gave an
overview of previous work in the field, and different datasets that are
available for the tasks.

In chapter \ref{ch:dataset} we introduced the ASK corpus, which we used for
the experiments in this thesis. We gave a short overview of previous studies
that have used the corpus to investigate features of Norwegian learner
language. We examined the distributions of metadata in the ASK corpus, and
created a training/development/test data split. We make the assignment to
different splits public so that others can replicate our experiments. See
table \ref{tab:topics-in-split} for split assignments.

In chapter \ref{ch:experiments} we examined a number of different evaluation
metrics that have previously been used in the \ac{AES} literature, and
decided on macro and micro \FI scores as the metrics we would use to evaluate
our models on the task. We explored a selection of different classifier
models, both linear and neural architectures. We compared three different
prediction modes, and saw that regression and ordinal regression had better
performance on the task than nominal classification. We established a strong
baseline using a \ac{SVM} regressor.

In chapter \ref{ch:sequencemodels} we performed experiments with more
advanced neural models, namely \acp{CNN} and \acp{RNN}. Our CNN experiments
confirmed what we observed in the previous chapter regarding nominal
classification versus regression, namely that regression gives the best
results for the \ac{AES} task on this dataset. Our \ac{RNN} experiments
resulted in the strongest results on the task using a bidirectional model
with GRU cells and an attention mechanism. The effects of random vs.
pre-trained initialization of embeddings, and whether \ac{POS} tags were used
as side input, were not observed to be significant.

We also tested our systems on the NLI task in isolation. We examined the
workings of the attention mechanism using heat map visualization. We did not
observe a correspondence between the parts of texts that our model focused on
and traits of learner language that is known from \ac{SLA} literature. We
plotted the representation vectors of documents in a 2-D projection, and saw
that the length of a document seemed to be considered important in our
system.

In chapter \ref{ch:multitask} we added \ac{NLI} as an auxiliary task to our
best models. We found that multi-task learning with NLI does not hurt
performance with small loss weights, but we could not find any consistent
improvement. We examined the correlation between different evaluation
metrics, and discovered that some of them have a non-linear relationship with
each other.

In chapter \ref{ch:heldout} we evaluated our models on a held-out test set we
created in chapter \ref{ch:dataset}, and found that the micro \FI results
were close to what we had seen on the development set, indicating that the
model generalized well to unseen data.


\section{Ethical considerations}

Language testing is a high-stakes setting. Often, an official language
proficiency certificate is needed in order to be admitted to a study
programme or gain employment, for instance. The high stakes involved means
that the grading of a language test should ideally be transparent and
explainable, something many modern machine learning based models struggle to
achieve. In the worst case, a person may fail an official language test that
is graded automatically, and have no way to know what aspects of the test
caused them to fail.

Even if a computer essay grading system is not used for grading an official
test, it may indirectly influence a language learner's decision to take a
test at a certain time. As established, language testing can be inconvenient
for those taking it, since they have to get to the testing location, pay a
fee, etc. For instance, if a language learner uses an automatic grading
system to find out what CEFR level they are at, and it concludes that they
are at a B2 level, while in fact the learner is still at a lower level, they
may choose to take a B2 level language test they are likely to fail. On the
other hand, if the grading system undershoots, the learner may waste time by
not taking the test at a point in time where they would already be ready for
it.

Language testing is in some places a requirement for gaining citizenship, a
practice that has faced scholarly criticism, for instance by
\textcite[162]{weberhorner}, who argue that language testing policies are a
means of social exclusion, rather than an opportunity that encourages
transnationals and migrants to learn the dominant language in the region they
have moved to. The practice has also been criticized for its application of
the \ac{CEFR}. For instance, \textcite{van2009fortress} finds it curious that
the required CEFR level varies between countries that use it for language
testing for citizenship, arguing that the variation reveals the practice as
arbitrary.


\section{Future work}

There are more architectures to explore for the \ac{AES} task on the ASK
corpus. For instance, the system in \textcite{alikaniotis2016automatic} gave
strong results on the ASAP dataset, and could be adapted to work with
documents from ASK.

Because documents can be long, it could prove beneficial to use a
hierarchical classifier. Each sentence in a document could be encoded by an
RNN, and a second RNN would then perform classification over a sequence of
sentence representations rather than a sequence of tokens.

The formulation of \ac{AES} as ordinal classification should be explored
further. It is a natural formulation of the task, at least with labels such
as \ac{CEFR} levels, where the number of different scores is reasonably low,
they have a clear order, but we do not know the distance between them. For
alternative scoring systems, such as numeric scores over a large interval,
regression may still be more appropriate.

The ASK corpus contains detailed annotations of mistakes with respect to the
target language. These could be compared to features discovered by \iac{ML}
system, in order to see to what degree it corresponds to linguistically
informed error features.
